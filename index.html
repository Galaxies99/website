<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongjie Fang</title>
  
  <meta name="author" content="Hongjie Fang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/icon.ico">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hongjie (Tony) Fang 方泓杰</name>
              </p>
              <p>
                I am a fourth-year Ph.D. student @ Computer Science in Wu Wenjun Honorable Class, <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University (SJTU)</a> & <a href="https://www.shlab.org.cn/">Shanghai Artificial Intelligence Laboratory</a>, advised by Prof. <a href="https://www.mvig.org/">Cewu Lu</a>. Previously, I got my B. Eng. degree @ Computer Science and Engineering, and B. Ec. degree @ Finance from SJTU in 2022.
                <br><br>
                My research interests mainly lie on robotic fields, specifically, robotic manipulation, robot learning and grasping. I am currently a member of <a href="https://www.mvig.org/">SJTU Machine Vision and Intelligence Group (MVIG)</a>. My goal is to enable robots to perform various tasks <i>in the real world</i>, improving the quality of human life.
                <br><br>
              </p>
              <p style="text-align:center">
                <a href="mailto:tony.fang.galaxies@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=Kyio_RAAAAAJ">GScholar</a> &nbsp/&nbsp
                <a href="https://github.com/galaxies99/">Github</a> &nbsp/&nbsp
                <a href="https://x.com/FangGalaxies">X</a> &nbsp/&nbsp
                <a href="images/profile/wechat.jpg">WeChat</a> &nbsp/&nbsp
                <a href="data/cv.pdf">CV</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%; text-align:center; vertical-align:middle;">
              <a href="images/profile/profile.jpg">
                <img style="width:75%;max-width:100%" alt="profile photo" src="images/profile/profile_circle.png" class="hoverZoomLink">
              </a>
              <br>
              <p>
                Photo @ İstanbul, Türkiye
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
                <b>[Aug. 2025]</b> <a href="https://airexo.tech/airexo2">AirExo-2</a> is accepted by CoRL 2025. See you in Seoul!<br>
                <b>[Jun. 2025]</b> Three papers (<a href="https://tonyfang.net/FoAR">FoAR</a>, <a href="https://ericjin2002.github.io/SIME/">SIME</a>, and <a href="https://knowledge-driven.github.io/">KDIL</a>) are accepted by IROS 2025. <br>
                <b>[Apr. 2025]</b> <a href="https://tonyfang.net/FoAR">FoAR</a> is accepted by RA-L. <br>
                <b>[Mar. 2025]</b> <a href="https://airexo.tech/airexo2">AirExo-2</a> is released! Check our website for more details.<br>
                <b>[Jan. 2025]</b> Two papers (<a href="https://tonyfang.net/s2i">S2I</a> and <a href="https://cage-policy.github.io/">CAGE</a>) are accepted by ICRA 2025. <br>
                <b>[Jun. 2024]</b> <a href="https://rise-policy.github.io/">RISE</a> is accepted by IROS 2024.<br>
                <b>[Jan. 2024]</b> Four papers (<a href="https://airexo.github.io/">AirExo</a>, <a href="https://rh20t.github.io/">RH20T</a>, <a href="https://robotics-transformer-x.github.io/">Open X-Embodiment</a> and <a href="https://graspnet.net/anygrasp.html">AnyGrasp</a>) are accepted by ICRA 2024. <br>
                <b>[Oct. 2023]</b> <a href="https://robotics-transformer-x.github.io/">Open X-Embodiment</a> is released! Proud of this wonderful collaboration in robotics community! <br>
                <b>[Sept. 2023]</b> <a href="https://airexo.github.io/">AirExo</a> is released! Check our website for more details.<br>
                <b>[Jun. 2023]</b> One paper is accepted by IROS 2023. <br>
                <b>[Apr. 2023]</b> <a href="https://graspnet.net/anygrasp/">AnyGrasp</a> is accepted by T-RO.<br>
                <b>[Feb, 2023]</b> One paper is accepted by CVPR 2023. <br>
                <b>[Feb, 2023]</b> <a href="https://ieeexplore.ieee.org/document/9796631">TransCG</a> is accepted by ICRA 2023 as RA-L submission. See you in London! <br>

                <b>[Jun, 2022]</b> <a href="https://ieeexplore.ieee.org/document/9796631">TransCG</a> is accepted by RA-L. <br>
                <b>[Aug, 2021]</b> One paper is accepted by ICCV 2021.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                Representative papers are <span class="highlight">highlighted</span>. 
                * denotes equal contribution. † denotes corresponding author(s).
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video src='history/resources/video/guess-hard/3-low.mp4' width="160" autoplay muted playsinline loop></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://tonyfang.net/history/">History-Aware Visuomotor Policy Learning via Point Tracking</a> </papertitle>
              <br>
              <a href="mailto:jjchen20@sjtu.edu.cn">Jingjing Chen*</a>,
              <strong>Hongjie Fang*</strong>,
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <a href="mailto:shiquan.wang@flexiv.com">Shiquan Wang†</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>arXiv</em>, 2025 
              <br>
              <a href="">paper comming soon</a> / <a href="">code comming soon</a> / <a href="https://tonyfang.net/history/">project page</a>
              <p></p>
              <p>
                Propose an object-centric history representation based on point tracking, which abstracts past observations into a compact and structured form that retains only essential task-relevant information. Tracked points are encoded and aggregated at the object level, yielding a compact history representation that can be seamlessly integrated into various visuomotor policies. Our design provides full history-awareness with high computational efficiency, leading to improved overall task performance and decision accuracy. Our history-aware policies consistently outperforms both Markovian baselines and prior history-based approaches.
              </p>
            </td>
          </tr>  

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <video src='images/research/dqrise.mp4' width="160" autoplay muted playsinline loop></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="http://rise-policy.github.io/DQ-RISE">Learning Dexterous Manipulation with Quantized Hand State</a> </papertitle>
              <br>
              <a href="mailto:fyyy0407@sjtu.edu.cn">Ying Feng*</a>,
              <strong>Hongjie Fang*</strong>,
              <a href="https://hyn-kulu.github.io/">Yinong He*</a>,
              <a href="mailto:jjchen20@sjtu.edu.cn">Jingjing Chen</a>,
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <a href="https://github.com/Alan-Heoooh">Zihao He</a>, 
              <a href="https://ruonanliu.com/">Ruonan Liu</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>arXiv</em>, 2025 
              <br>
              <a href="">paper comming soon</a> / <a href="">code comming soon</a> / <a href="http://rise-policy.github.io/DQ-RISE">project page</a>
              <p></p>
              <p>
                Propose an object-centric history representation based on point tracking, which abstracts past observations into a compact and structured form that retains only essential task-relevant information. Tracked points are encoded and aggregated at the object level, yielding a compact history representation that can be seamlessly integrated into various visuomotor policies. Our design provides full history-awareness with high computational efficiency, leading to improved overall task performance and decision accuracy. Our history-aware policies consistently outperforms both Markovian baselines and prior history-based approaches.
              </p>
            </td>
          </tr>  

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/airexo2.gif' width="160"></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://airexo.tech/airexo2">AirExo-2: Scaling up Generalizable Robotic Imitation Learning with Low-Cost Exoskeletons
              </a> </papertitle>
              <br>
              <strong>Hongjie Fang*</strong>,
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang*</a>,
              <a href="mailto:sommerfeld@sjtu.edu.cn">Yiming Wang*</a>,
              <a href="mailto:jjchen20@sjtu.edu.cn">Jingjing Chen*</a>,
              <a href="https://github.com/Xiashangning">Shangning Xia</a>, 
              <a href="https://lyuj1998.github.io/">Jun Lv</a>,
              <a href="https://github.com/Alan-Heoooh">Zihao He</a>, 
              <a href="mailto:simonyxy@sjtu.edu.cn">Xiyan Yi</a>,
              <a href="mailto:yunhan_guo@sjtu.edu.cn">Yunhan Guo</a>,
              <a href="https://github.com/kelvin34501">Xinyu Zhan</a>, 
              <a href="https://lixiny.github.io/">Lixin Yang</a>,
              <a href="mailto:wangweiming@sjtu.edu.cn">Weiming Wang</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang†</a>
              <br>
              <em>CoRL</em>, 2025 <b><text style="color:red">(oral)</text></b> &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2503.03081">paper</a> / <a href="https://github.com/AirExo/AirExo-2">data collection code</a> / <a href="https://github.com/rise-policy/RISE-2/">policy code</a> / <a href="https://airexo.tech/airexo2">project page</a>
              <p></p>
              <p>
                Develop <i>AirExo</i>-2, an updated low-cost exoskeleton system for large-scale in-the-wild demonstration collection. By transforming the collected in-the-wild demonstrations into pseudo-robot demonstrations, our system addresses key challenges in utilizing in-the-wild demonstrations for downstream imitation learning in the real world. Propose <i>RISE</i>-2, a generalizable imitation policy that integrates 2D and 3D perceptions, outperforming previous imitation learning policies in both in-domain and out-of-domain tasks, even with limited demonstrations. By leveraging in-the-wild demonstrations collected and transformed by the <i>AirExo</i>-2 system, without the need for additional robot demonstrations, <i>RISE</i>-2 achieves comparable or superior performance to policies trained with teleoperated data, highlighting the potential of <i>AirExo</i>-2 for scalable and generalizable imitation learning.
              </p>
            </td>
          </tr>   


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/anydexgrasp.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://graspnet.net/anydexgrasp/">AnyDexGrasp: General Dexterous Grasping for Different Hands with Human-Level Learning Efficiency</a> </papertitle>
              <br>
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang†</a>,
              <a href="https://github.com/HXY-95/">Hengxu Yan</a>, 
              <a href="https://github.com/Vladimirovich2019">Zhenyu Tang</a>,
              <strong>Hongjie Fang</strong>, 
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>arXiv</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2502.16420">paper</a> / <a href="https://graspnet.net/anydexgrasp/">project page</a>
              <p></p>
              <p>
                Introduce AnyDexGrasp, an efficient approach for learning dexterous grasping with minimal data, advancing robotic
                manipulation capabilities across different robotic hands. Our results show a grasp success rate of 75-95% across three different robotic hands in real-world cluttered
                environments with over 150 novel objects, improving to 80-98% with increased training objects.
              </p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/kdil.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://knowledge-driven.github.io/">Knowledge-Driven Imitation Learning: Enabling Generalization Across Diverse Conditions</a> </papertitle>
              <br>
              <a href="https://github.com/mioam">Zhuochen Miao*</a>,
              <a href="https://lyuj1998.github.io/">Jun Lv*</a>,
              <strong>Hongjie Fang</strong>, 
              <a href="https://github.com/EricJin2002">Yang Jin</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>IROS</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2506.21057">paper</a> / <a href="https://github.com/mioam/KnowledgeIL">code</a> / <a href="https://knowledge-driven.github.io/">project page</a>
              <p></p>
              <p>
                We propose knowledge-driven imitation learning, a framework that leverages external structural semantic knowledge to abstract object representations within the same category during imitation learning. We introduce a novel semantic keypoint graph as a knowledge template and develop a coarse-to-fine template-matching algorithm that optimizes both structural consistency and semantic similarity. 
              </p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/sime.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://ericjin2002.github.io/SIME/">SIME: Enhancing Policy Self-Improvement with Modal-Level Exploration</a> </papertitle>
              <br>
              <a href="https://github.com/EricJin2002">Yang Jin*</a>,
              <a href="https://lyuj1998.github.io/">Jun Lv*</a>,
              <a href="https://virlus.github.io/">Wenye Yu</a>,
              <strong>Hongjie Fang</strong>, 
              <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>IROS</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2505.01396">paper</a> / <a href="https://github.com/EricJin2002/SIME">code</a> / <a href="https://ericjin2002.github.io/SIME/">project page</a>
              <p></p>
              <p>
                We found that with modal-level exploration, the robot can generate more diverse and multi-modal interaction data. By learning from the most valuable trials and high-quality segments from these interactions, the robot can effectively refine its capabilities through self-improvement.
              </p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/dsp.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://selen-suyue.github.io/DspNet/">Dense Policy: Bidirectional Autoregressive Learning of Actions</a> </papertitle>
              <br>
              <a href="https://selen-suyue.github.io/">Yue Su*</a>,
              <a href="https://github.com/kelvin34501">Xinyu Zhan*</a>, 
              <strong>Hongjie Fang</strong>, 
              <a href="https://hanxue.me/">Han Xue</a>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>,
              <a href="https://www.mvig.org/">Cewu Lu</a>,
              <a href="https://lixiny.github.io/">Lixin Yang†</a>
              <br>
              <em>ICCV</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2503.13217">paper</a> / <a href="https://github.com/Selen-Suyue/DensePolicy">code</a> / <a href="https://selen-suyue.github.io/DspNet/">project page</a>
              <p></p>
              <p>
                Propose a bidirectionally expanded learning approach that enhances auto-regressive policies for robotic manipulation. It employs a lightweight encoder-only architecture to iteratively unfold the action sequence from an initial single frame into the target sequence in a coarse-to-fine manner with logarithmic-time inference.
              </p>
            </td>
          </tr>  

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/foar.gif' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://tonyfang.net/FoAR/">FoAR: Force-Aware Reactive Policy for Contact-Rich Robotic Manipulation</a> </papertitle>
              <br>
              <a href="https://github.com/Alan-Heoooh">Zihao He*</a>, 
              <strong>Hongjie Fang*</strong>, 
              <a href="mailto:jjchen20@sjtu.edu.cn">Jingjing Chen</a>, 
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang†</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>RA-L</em>, 2025 &nbsp <br> 
              <em>IROS</em>, 2025
              <br>
              <a href="https://arxiv.org/pdf/2411.15753">paper</a> / <a href="https://github.com/Alan-Heoooh/FoAR">code</a> / <a href="https://tonyfang.net/FoAR/">project page</a> / <a href="https://x.com/FangGalaxies/status/1861214958517936146">X</a>
              <p></p>
              <p>
                Propose FoAR, a force-aware reactive policy that combines high-frequency force/torque sensing with visual inputs to enhance the performance in contact-rich manipulation. Built upon the RISE policy, FoAR incorporates a multimodal feature fusion mechanism guided by a future contact predictor, enabling dynamic adjustment of force/torque data usage between non-contact and contact phases. Its reactive control strategy also allows FoAR to accomplish contact-rich tasks accurately through simple position control.
              </p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/mba.gif' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://selen-suyue.github.io/MBApage/">Motion Before Action: Diffusing Object Motion as Manipulation Condition</a> </papertitle>
              <br>
              <a href="https://selen-suyue.github.io/">Yue Su*</a>,
              <a href="https://github.com/kelvin34501">Xinyu Zhan*</a>, 
              <strong>Hongjie Fang</strong>, 
              <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a>,
              <a href="https://www.mvig.org/">Cewu Lu</a>,
              <a href="https://lixiny.github.io/">Lixin Yang†</a>
              <br>
              <em>RA-L</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2411.09658">paper</a> / <a href="https://github.com/Selen-Suyue/MBA">code</a> / <a href="https://selen-suyue.github.io/MBApage/">project page</a>
              <p></p>
              <p>
                Propose MBA, a novel module that employs two cascaded diffusion processes for object motion generation and robot action generation under object motion guidance. Designed as a plug-and-play component, MBA can be flexibly integrated into existing robotic manipulation policies with diffusion action heads. Extensive experiments in both simulated and real-world environments demonstrate that our approach substantially improves the performance of existing policies across a wide range of manipulation tasks.
              </p>
            </td>
          </tr>  

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/cage.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://cage-policy.github.io/">CAGE: Causal Attention Enables Data-Efficient Generalizable Robotic Manipulation</a> </papertitle>
              <br>
              <a href="https://github.com/Xiashangning">Shangning Xia</a>, 
              <strong>Hongjie Fang</strong>, 
              <a href="https://www.mvig.org/">Cewu Lu†</a>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang†</a>
              <br>
              <em>ICRA</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2410.14974">paper</a> / <a href="https://github.com/cage-policy/cage"> code</a> / <a href="https://cage-policy.github.io/">project page</a>
              <p></p>
              <p>
                Introduce CAGE, a data-efficient generalizable robotic manipulation policy. With less than 50 demonstrations in the mono-distributed training environment, CAGE can effectively generalize to similar and unseen environments with different levels of distribution shifts (background, object and camera view changes), outperforming previous state-of-the-art policies. This work makes a step forward in developing data-efficient, scalable, and generalizable robotic manipulation policies.
              </p>
            </td>
          </tr>  

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/s2i.jpg' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://tonyfang.net/s2i">Towards Effective Utilization of Mixed-Quality Demonstrations in Robotic Manipulation via Segment-Level Selection and Optimization</a> </papertitle>
              <br>
              <a href="mailto:jjchen20@sjtu.edu.cn">Jingjing Chen</a>, 
              <strong>Hongjie Fang</strong>, 
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>ICRA</em>, 2025 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2409.19917">paper</a> / <a href="https://github.com/junxix/s2i">code</a> / <a href="https://tonyfang.net/s2i">project page</a>
              <p></p>
              <p>
                Introduce S2I, a framework that selects and optimizes mixed-quality demonstration data at the segment level, while ensuring plug-and-play compatibility with existing robotic manipulation policies. With only 3 expert demonstrations for reference, S2I can improve the performance of various downstream policies when trained with mixed-quality demonstrations.
              </p>
            </td>
          </tr>  

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/rise.jpg' width="160"></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://rise-policy.github.io/">RISE: 3D Perception Makes Real-World Robot Imitation Simple and Effective</a> </papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <strong>Hongjie Fang</strong>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang†</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>IROS</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2404.12281.pdf"> paper </a> / 
              <a href="https://github.com/rise-policy/RISE">code</a> / <a href="https://rise-policy.github.io/">project page</a> / <a href="https://x.com/FangGalaxies/status/1792949268179005564">X</a>
              <p></p>
              <p>
                Propose <i>RISE</i>, an end-to-end baseline for real-world robot imitation learning, which predicts continuous actions directly from single-view point clouds. Trained with 50 demonstrations for each real-world task, RISE surpasses currently representative 2D and 3D policies by a large margin, showcasing significant advantages in both accuracy and efficiency.
              </p>
            </td>
          </tr>   

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/oxe.gif' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://robotics-transformer-x.github.io/">Open X-Embodiment: Robotic Learning Datasets and RT-X Models</a> </papertitle>
              <br>
              Open X-Embodiment Collaboration, [...], <strong>Hongjie Fang</strong>, [...] (194 authors)
              <br>
              <em>ICRA</em>, 2024  <b><text style="color:red">(best paper)</text></b> &nbsp 
              <br>
              <a href="https://robotics-transformer-x.github.io/paper.pdf">paper</a> / <a href="https://robotics-transformer-x.github.io/">project page</a> / <a href="https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?usp=sharing">dataset</a>
              <p></p>
              <p>
                Introduce the Open X-Embodiment Dataset, the largest robot learning dataset to date with 1M+ real robot trajectories, spanning 22 robot embodiments. Train large, transformer-based policies on the dataset (RT-1-X, RT-2-X) and show that co-training with our diverse dataset substantially improves performance.
              </p>
            </td>
          </tr>   


          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/airexo.gif' width="160"></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://airexo.github.io/">AirExo: Low-Cost Exoskeletons for Learning Whole-Arm Manipulation in the Wild</a> </papertitle>
              <br>
              <strong>Hongjie Fang*</strong>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang*</a>,
              <a href="mailto:sommerfeld@sjtu.edu.cn">Yiming Wang*</a>,
              <a href="mailto:jiejiren@sjtu.edu.cn">Jieji Ren</a>,
              <a href="mailto:jjchen20@sjtu.edu.cn">Jingjing Chen</a>,
              <a href="mailto:ruozhang0608@gmail.com">Ruo Zhang</a>,
              <a href="mailto:wangweiming@sjtu.edu.cn">Weiming Wang</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>ICRA</em>, 2024
              <br>
              <a href="https://arxiv.org/pdf/2309.14975.pdf"> paper </a> / <a href="https://airexo.github.io/">project page</a> / <a href="https://x.com/haoshu_fang/status/1707434624413306955">X</a>
              <p></p>
              <p>
                Develop <i>AirExo</i>, a low-cost,
                adaptable, and portable dual-arm exoskeleton, for joint-level teleoperation and demonstration collection. Further leverage <i>AirExo</i> for learning with cheap demonstrations in the wild to improve sample efficiency and robustness of the policy.
              </p>
            </td>
          </tr>   

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/rh20t.gif' width="160"></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://rh20t.github.io/">RH20T: A Comprehensive Robotic Dataset for Learning Diverse Skills in One-Shot</a> </papertitle>
              <br>
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <strong>Hongjie Fang</strong>,
              <a href="https://github.com/Vladimirovich2019">Zhenyu Tang</a>,
              <a href="https://github.com/todibo99">Jirong Liu</a>,              
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <a href="https://github.com/dadadadawjb">Junbo Wang</a>,
              <a href="https://www.haoyizhu.site/">Haoyi Zhu</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>ICRA</em>, 2024 
              <br>
              <a href="https://openreview.net/forum?id=YhRKICWgE9"> paper </a> / <a href="https://github.com/rh20t/rh20t_api">API</a> / <a href="https://rh20t.github.io/">project page</a> / <a href="https://x.com/haoshu_fang/status/1706881232548814899">X</a>
              <p></p>
              <p>
                Collect a dataset comprising over 110k contact-rich robot manipulation sequences across diverse skills, contexts, robots, and camera viewpoints, all collected in the real world. Each sequence in the dataset includes visual, force, audio, and action information, along with a corresponding human demonstration video. Put significant efforts in calibrating all the sensors and ensures a high-quality dataset.
              </p>
            </td>
          </tr>   

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/handover.png' width="160"></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://arxiv.org/abs/2308.15622">Flexible Handover with Real-Time Robust Dynamic Grasp Trajectory Generation</a> </papertitle>
              <br>
              <a href="https://github.com/blakery-star">Gu Zhang</a>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <strong>Hongjie Fang</strong>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>IROS</em>, 2023 &nbsp 
              <br>
              <a href="https://arxiv.org/pdf/2308.15622">paper</a> 
              <p></p>
              <p>
                Propose an approach for effective and robust flexible handover, which enables the robot to grasp moving objects with flexible motion trajectories with a high success rate. The key innovation of our approach is the generation of real-time robust grasp trajectories. Designs a future grasp prediction algorithm to enhance the system's adaptability to dynamic handover scenes.
              </p>
            </td>
          </tr>   

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/reactive.gif' width="160"></div>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://graspnet.net/reactive">Target-Referenced Reactive Grasping for Dynamic Objects</a></papertitle>
              <br>
              <a href="https://github.com/todibo99">Jirong Liu</a>,
              <a href="mailto:ruozhang0608@gmail.com">Ruo Zhang</a>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <a href="https://gouminghao.github.io/">Minghao Gou</a>,
              <strong>Hongjie Fang</strong>,
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <a href="https://github.com/XSE42">Sheng Xu</a>,
              <a href="https://github.com/HXY-95/">Hengxu Yan</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>CVPR</em>, 2023 &nbsp 
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Target-Referenced_Reactive_Grasping_for_Dynamic_Objects_CVPR_2023_paper.pdf"> paper </a> /
              <a href="https://graspnet.net/reactive"> project page </a> 
              <p></p>
              <p>
                Focus on semantic consistency instead of temporal smoothness of the predicted grasp poses during reactive grasping. Solve the reactive grasping problem in a target-referenced setting by tracking through generated grasp spaces.
              </p>
            </td>
          </tr>   

          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/anygrasp.gif' width="160"></div>

              <img src='images/research/anygrasp-fish.gif' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://graspnet.net/anygrasp/">
                <papertitle>AnyGrasp: Robust and Efficient Grasp Perception in Spatial and Temporal Domains</papertitle>
              </a>
              <br>
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <strong>Hongjie Fang</strong>,
              <a href="https://gouminghao.github.io/">Minghao Gou</a>,
              <a href="https://github.com/todibo99">Jirong Liu</a>,
              <a href="https://github.com/HXY-95/">Hengxu Yan</a>,
              <a href="https://scholar.google.com/citations?user=G7fiO-UAAAAJ">Wenhai Liu</a>,
              <a href="https://scholar.google.com/citations?user=SdX6DaEAAAAJ">Yichen Xie</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>T-RO</em>, 2023 &nbsp <br>
              <em>ICRA</em>, 2024 &nbsp
              <br>
              <a href="https://arxiv.org/pdf/2212.08333">paper</a> / 
              <a href="https://github.com/graspnet/anygrasp_sdk">SDK</a> /
              <a href="https://graspnet.net/anygrasp/">project page</a> 
              <p></p>
              <p>
                Propose a powerful <i>AnyGrasp</i> model for general grasping, including static scenes and dynamic scenes. <i>AnyGrasp</i> can generate accurate, full-DoF, dense and temporally-smooth grasp poses efficiently, and it works robustly against large depth sensing noise. 
              </p>
            </td>
          </tr>   



          <tr bgcolor="#ffffd0">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/transcg.gif' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://graspnet.net/transcg/">
                <papertitle>TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and a Grasping Baseline</papertitle>
              </a>
              <br>
              <strong>Hongjie Fang</strong>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <a href="https://github.com/XSE42">Sheng Xu</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>RA-L</em>, 2022 &nbsp
              <br>
              <em>ICRA</em>, 2023 &nbsp
              <br>
              <a href="https://ieeexplore.ieee.org/document/9796631">paper</a> / 
              <a href="https://github.com/galaxies99/transcg">code</a> /
              <a href="https://graspnet.net/transcg/">project page </a> 
              <p></p>
              <p>
                Propose <i>TransCG</i>, a large-scale real-world dataset for transparent object depth completion, along with a depth completion method <i>DFNet</i> based on the <i>TransCG</i> dataset.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/research/graspness.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Graspness_Discovery_in_Clutters_for_Fast_and_Accurate_Grasp_Detection_ICCV_2021_paper.html">
                <papertitle>Graspness Discovery in Clutters for Fast and Accurate Grasp Detection</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=bK1fWXcAAAAJ">Chenxi Wang</a>,
              <a href="https://fang-haoshu.github.io/">Hao-Shu Fang</a>,
              <a href="https://gouminghao.github.io/">Minghao Gou</a>,
              <strong>Hongjie Fang</strong>,
              <a href="https://scholar.google.com/citations?hl=en&user=HkVy8voAAAAJ">Jin Gao</a>,
              <a href="https://www.mvig.org/">Cewu Lu†</a>
              <br>
              <em>ICCV</em>, 2021 &nbsp 
              <br>
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Graspness_Discovery_in_Clutters_for_Fast_and_Accurate_Grasp_Detection_ICCV_2021_paper.pdf">paper</a>
              <p></p>
              <p>
                Propose <i>graspness</i>, a quality based on geometry cues that distinguishes graspable area in cluttered scenes, which can be measured by a look-ahead searching method. Propose a <i>graspness</i> model to approximate the <i>graspness</i> value for quickly detect grasps in practice. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Projects</heading>
            </td>
          </tr>
        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/project/easyrobot.png' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://github.com/Galaxies99/easyrobot">EasyRobot</a></papertitle>
              <br>
              <b>Hongjie Fang</b>
              <br>
              <em>research project</em>, <text style="color:red">under active development</text>
              <br>
              <a href="https://github.com/Galaxies99/easyrobot">code</a>
              <p>
                Provides an easy and unified interface for robots, grippers, sensors and pedals.
              </p>
            </td>
          </tr>
          
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src='images/project/oh-my-papers.gif' width="160"></div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle><a href="https://github.com/Galaxies99/oh-my-papers">Oh-My-Papers</a></papertitle>
              <br>
              <b>Hongjie Fang</b>, <a href="https://zhandazhu.com/">Zhanda Zhu</a>, <a href="https://github.com/zhao-hr">Haoran Zhao</a>
              <br>
              <em>course project of SJTU undergraduate course "Mobile Internet"</em>
              <br>
              <a href="https://github.com/Galaxies99/oh-my-papers">code</a> / <a href="https://github.com/zhao-hr/oh-my-papers-website">demo</a> / <a href="https://github.com/Galaxies99/oh-my-papers/blob/main/assets/Oh-My-Papers%20a%20Hybrid%20Context-aware%20Paper%20Recommendation%20System.pdf">report</a>
              <p>
                Proposes that we can learn "jargons" like "ResNet" and "YOLO" from academic paper citation information, and such citation information can be regarded as the searching results of the corresponding "jargon". For example, when searching "ResNet", the engine should return the "Deep Residual Learning for Image Recognition", instead of the papers that contains word "ResNet" in their titles, as current scholar search engines commonly return.
              </p>
            </td>
          </tr>   
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Services</heading>
              <p>
                <b>Reviewer for Conferences</b>: ICRA 2023/2024/2025, IROS 2023/2024/2025, ICLR 2025, CoRL 2025, NeurIPS 2025.
                <b>Reviewer for Journals</b>: RA-L, T-CYB, T-MECH.
              </p>
              <!---
              <p>
                <b>Teaching Assistant</b>:
                <ul>
                <li>Data Structure (Honor). Instructor: <a href = "http://apex.sjtu.edu.cn/members/yyu">Prof. Yong Yu</a>. Fall 2019. <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href = "https://github.com/peterzheng98/CS158-DS_Project">[Course Project]</a></li>
                <li> Algorithm and Complexity. Instructor: <a href="https://www.cs.sjtu.edu.cn/~gao-xf/">Prof. Xiaofeng Gao</a>. Spring 2022. <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://anl.sjtu.edu.cn/gao-xf/course/CS214-2022">[Course Page]</a></li>
                <li> Linear Algebra (Honor). Instructor: <a href = "https://zhiyuan.sjtu.edu.cn/html/zhiyuan/faculty_view.php?id=482">Prof. Hao Shen</a>. Fall 2022; Fall 2021. <br> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://jbox.sjtu.edu.cn/v/link/view/ab55cf1655334d709b6a2cb029b2db15">[Exercise Class Materials]</a></li>
                <li>C++ Programming Language (Honor). Instructor: <a href="https://www.cs.sjtu.edu.cn/PeopleDetail.aspx?id=78">Prof. Huiyu Weng</a>. Fall 2020.</li>
                <li> Mathematical Analysis (Honor). Instructor: <a href="https://math.sjtu.edu.cn/Default/teachershow/tags/MDAwMDAwMDAwMLJ4mJc">Prof. Keying Chen</a>. Fall 2021; Fall 2020.</li>
                </ul>
              </p>
            -->
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Invited Talks</heading>
              <p>
                <ul>
                  <li> <b>[Mar. 2024]</b> <a href="https://www.echoaitalk.com/work/s1e07">Echo AI Talk</a>. Towards Efficient Robot Imitation Learning from Human Demonstrations. <a href="https://www.youtube.com/watch?v=46YvkX_a1Ig">[Replay]</a></li>
                  <li> <b>[Nov. 2024]</b> <a href="https://course.zhidx.com/">Zhixingxing</a>. Towards Efficient Robot Imitation Learning from Human Demonstrations. <a href="https://www.bilibili.com/video/BV1gwiBYXErh">[Replay]</a></li>
                  <li> <b>[Mar. 2025]</b> <a href="https://people.iiis.tsinghua.edu.cn/~gaoyang/yang-gao.weebly.com/index.html">THU Yang Gao Group</a>. Towards Generalizable Imitation Learning from Human Demonstrations. Thanks <a href="https://alvinwen428.github.io/">Chuan Wen</a> for invitation. </li>
                  <li> <b>[Aug. 2025]</b> <a href="https://www.3dcver.com/">3DCVer</a>. Towards Generalizable Imitation Learning from Human Demonstrations. <a href="https://www.bilibili.com/video/BV1uwtPz1EzZ">[Replay]</a></li>
                  <li> <b>[Aug. 2025]</b> <a href="https://www.sharpa.com/">Sharpa</a>. Towards Generalizable Imitation via Scalable Data and Robust Policy. Thanks <a href="https://openreview.net/profile?id=~Kaifeng_Zhang1">Kaifeng Zhang</a> for invitation. </li>
                  </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>More about Me</heading>
              <p>
                <b> Some of My Notes </b> ---> <a href="https://tonyfang.net/notes">Notes</a>
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small">
                <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Ftonyfang.net&count_bg=%233366DD&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false"/></a>
              </p>
              <p style="text-align:right;font-size:small;">
                The website is built upon this <a href="https://github.com/jonbarron/jonbarron_website">template</a>. 
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>